# =============================================================================
# Azure OpenAI Model Migration - Configuration Settings
# =============================================================================
# Copy this file to settings.yaml and update with your credentials

azure:
  # Azure OpenAI / AI Foundry Configuration
  endpoint: "${AZURE_OPENAI_ENDPOINT}"  # Use environment variable
  # Authentication: Entra ID via DefaultAzureCredential (recommended).
  # In Azure Container Apps the User-Assigned Managed Identity is used
  # automatically via the AZURE_CLIENT_ID env var set by Bicep.
  # For local development, run 'az login' — no API key needed.
  # To use an API key as fallback, set AZURE_OPENAI_API_KEY env var.
  api_key: "${AZURE_OPENAI_API_KEY}"    # Use environment variable
  api_version: "2025-04-01-preview"
  
  # Model Deployments
  # Each model needs a unique key (used as directory name and API identifier).
  # model_family determines prompt-style guidance (e.g. "gpt4", "gpt5").
  models:
    gpt4:
      deployment_name: "gpt-4.1"
      model_family: "gpt4"
      model_version: "2024-08-06"
      max_tokens: 4096
      temperature: 0.1  # Low for reproducibility

    gpt4o:
      deployment_name: "gpt-4o"
      model_family: "gpt4"
      model_version: "2024-08-06"
      max_tokens: 4096
      temperature: 0.1

    gpt41_mini:
      deployment_name: "gpt-4.1-mini"
      model_family: "gpt4"
      model_version: "2025-04-14"
      max_tokens: 16384
      temperature: 0.1

    gpt5:
      deployment_name: "gpt-5.2"
      model_family: "gpt5"
      model_version: "2025-01-01"
      max_tokens: 8192
      temperature: 0.1
      
    gpt5_reasoning:
      deployment_name: "gpt-5.1"
      model_family: "gpt5"
      model_version: "2025-01-01"
      max_tokens: 16384
      reasoning_effort: "medium"  # low, medium, high

# =============================================================================
# Microsoft Foundry Control Plane Evaluation (optional)
# =============================================================================
foundry:
  # Project endpoint from Azure AI Foundry portal → Project → Overview
  project_endpoint: "${FOUNDRY_PROJECT_ENDPOINT}"   # e.g. https://<name>.services.ai.azure.com/api/projects/<project>
  # Deployment used as LLM judge for quality evaluations
  judge_deployment: "gpt-5.2"   # e.g. gpt-4.1 (must be deployed in the Foundry project)
  # Model used inside score_model graders (must be a supported grader model:
  # gpt-4.1, gpt-4o, gpt-4.1-mini, o3-mini, o4-mini, etc.)
  grader_model: "gpt-4.1"
  # Include safety evaluators (safety_violence, safety_hate_unfairness).
  # These prompts can trigger Azure OpenAI content filters and cause the
  # Foundry internal eval run to fail with InternalServerError.
  # If Foundry evals fail repeatedly, try setting this to false.
  include_safety_evaluators: true
  # Timeout (seconds) for each Foundry eval run polling cycle.
  # With 7 evaluators × 20 rows and two models running in parallel,
  # runs typically complete in 150-350s.  Default: 600.
  timeout: 600

# =============================================================================
# Evaluation Settings
# =============================================================================
evaluation:
  # Number of runs per test case for consistency measurement
  consistency_runs: 5
  
  # Timeout settings (seconds)
  request_timeout: 60
  total_timeout: 300
  
  # Batch processing
  batch_size: 10
  parallel_requests: 5
  
  # Performance — parallel execution
  # max_concurrent_requests: semaphore limit for async API calls within a
  #   single evaluation.  Tune based on your Azure OpenAI RPM/TPM tier.
  max_concurrent_requests: 5
  # parallel_models: when comparing two models, evaluate both simultaneously
  parallel_models: true
  
  # Output settings
  results_dir: "data/results"
  save_responses: true
  
  # Metrics to compute
  metrics:
    - accuracy
    - consistency
    - latency
    - token_usage
    - cost

  # Acceptance thresholds — PASS / FAIL gates for migration readiness
  acceptance_thresholds:
    classification:
      accuracy: 0.90
      consistency: 0.85
      max_latency_ms: 3000
    dialog:
      quality_score: 0.80
      consistency: 0.80
      max_latency_ms: 5000
    rag:
      groundedness: 0.85
      relevance: 0.80
      max_latency_ms: 5000
    tool_calling:
      tool_selection_accuracy: 0.90
      parameter_accuracy: 0.85
      max_latency_ms: 4000
    general:
      quality_score: 0.75
      max_latency_ms: 5000

# =============================================================================
# Caching Configuration
# =============================================================================
caching:
  enabled: true
  cache_dir: ".cache/prompts"
  ttl_seconds: 86400  # 24 hours
  
  # Prompt caching for Azure OpenAI
  prompt_caching:
    enabled: true
    min_prefix_length: 1024  # Minimum tokens for cache benefit

# =============================================================================
# Security Settings
# =============================================================================
security:
  # Content filtering
  content_filter:
    enabled: true
    categories:
      - hate
      - sexual
      - violence
      - self_harm
  
  # PII handling
  pii_detection:
    enabled: true
    redact_in_logs: true
    
  # Sandbox tools
  code_interpreter:
    enabled: false  # Enable only when needed
    timeout: 30
    
# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  level: "INFO"
  format: "json"
  file: "logs/migration_eval.log"
  
  # Diagnostic logging for SDK
  sdk_diagnostics:
    enabled: true
    log_threshold_ms: 1000  # Log slow requests

# =============================================================================
# Web Interface Settings
# =============================================================================
web:
  host: "127.0.0.1"
  port: 5000
  debug: false
  
  # CORS settings
  cors:
    origins:
      - "http://localhost:3000"
      - "http://127.0.0.1:5000"
