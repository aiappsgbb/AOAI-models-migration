#!/usr/bin/env python3
"""
import_topic.py — Import an external topic into the evaluation solution
=======================================================================

Creates an archived topic inside the solution structure
(prompts/topics/{slug}/ and data/synthetic/topics/{slug}/) from:
  - A topic name
  - One or more source prompt text files
  - One or more JSON test data files

The source model is configurable (defaults to the first configured model), and prompts are
auto-generated for all configured target models (or a custom subset).

Source prompt args determine the task type:
  --source-class-prompt         ->  classification
  --source-dialog-prompt        ->  dialog
  --source-rag-prompt           ->  rag
  --source-tool-calling-prompt  ->  tool_calling

Legacy --gpt4-* aliases are supported for backward compatibility with older scripts.

The topic is ready to be activated from the web UI to run evaluations and
comparisons just like any other topic generated by the solution.

Steps performed:
  1. Validates each source prompt, ensuring it has the output format
     required by the evaluation pipeline (adds it if missing).
  2. Generates an optimised prompt for each target model per source prompt.
  3. Validates test data and fills in optional missing fields.
  4. Writes everything as an archived topic in the solution structure.

Does NOT modify active files, run evaluations, or generate reports.
All of that is done afterwards from the web UI.

Usage
-----
    # Import with explicit source/target models
    python tools/import_topic.py ^
        --topic "Insurance Claims Processing" ^
        --source-model gpt4 ^
        --target-models gpt5,gpt4o,gpt41_mini ^
        --source-class-prompt my_cls_prompt.txt ^
        --class-test-data classification_data.json ^
        --general-test-data general_data.json

    # Legacy syntax (still works)
    python tools/import_topic.py ^
        --topic "Hotel Concierge" ^
        --gpt4-dialog-prompt hotel_prompt.txt ^
        --dialog-test-data hotel_scenarios.json
"""

from __future__ import annotations

import argparse
import asyncio
import json
import logging
import os
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# ---------------------------------------------------------------------------
# Ensure the project root is importable regardless of cwd
# ---------------------------------------------------------------------------
_SCRIPT_DIR = Path(__file__).resolve().parent
_PROJECT_ROOT = _SCRIPT_DIR.parent
sys.path.insert(0, str(_PROJECT_ROOT))

from src.clients.azure_openai import create_client_from_config, AzureOpenAIClient
from src.utils.prompt_manager import _extract_categories_from_prompt, _slugify
from src.utils.model_guidance import get_guidance as _get_model_guidance, resolve_model_family as _resolve_model_family

# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------
CONFIG_PATH = str(_PROJECT_ROOT / "config" / "settings.yaml")
GENERATOR_MODEL = "gpt5"

TASK_PROMPT_MAP = {
    "classification": "classification_agent_system",
    "dialog":         "dialog_agent_system",
    "rag":            "rag_agent_system",
    "tool_calling":   "tool_calling_agent_system",
}

DATA_FILE_MAP = {
    "classification": "classification_scenarios.json",
    "dialog":         "follow_up_scenarios.json",
    "general":        "capability_tests.json",
    "rag":            "rag_scenarios.json",
    "tool_calling":   "tool_calling_scenarios.json",
}

# Archive directories inside the solution
PROMPTS_TOPICS_DIR = _PROJECT_ROOT / "prompts" / "topics"
DATA_TOPICS_DIR    = _PROJECT_ROOT / "data" / "synthetic" / "topics"

# ---------------------------------------------------------------------------
# Logging
# ---------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s  %(levelname)-5s  %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
log = logging.getLogger("import_topic")


# ===================================================================
# Source prompt validation — ensure evaluation-compatible output format
# ===================================================================

_CLASSIFICATION_OUTPUT_BLOCK = """

---
## MANDATORY OUTPUT FORMAT

You MUST return a single JSON object (no markdown fences, no extra text) with
this exact structure:

```json
{
  "category": "<primary_category_code>",
  "subcategory": "<subcategory_code>",
  "priority": "<low|medium|high|critical>",
  "sentiment": "<sentiment_value>",
  "confidence": <0.0-1.0>,
  "entities": {
    "names": [],
    "ids": [],
    "amounts": [],
    "dates": [],
    "products": [],
    "other": []
  },
  "follow_up_questions": [],
  "reasoning_summary": "<one-sentence explanation>"
}
```

Return ONLY this JSON object. No additional text before or after.
"""

_DIALOG_OUTPUT_BLOCK = """

---
## RESPONSE GUIDELINES

When responding to the customer:
1. Begin with an empathetic acknowledgment of the customer's situation.
2. Address the specific question or concern directly.
3. Ask targeted follow-up questions to fill any information gaps.
4. Provide actionable next steps where possible.
5. Maintain a professional, warm tone throughout.
"""

_RAG_OUTPUT_BLOCK = """

---
## RAG RESPONSE GUIDELINES

When answering based on retrieved context:
1. Ground ALL claims in the provided context — cite specific passages.
2. If the context does not contain sufficient information, state that explicitly.
3. NEVER fabricate or hallucinate information beyond the context.
4. Synthesize information from multiple context passages when relevant.
5. Indicate confidence level when context is ambiguous or incomplete.
"""

_TOOL_CALLING_OUTPUT_BLOCK = """

---
## TOOL CALLING GUIDELINES

When selecting and invoking tools:
1. Select the most appropriate tool(s) based on the user's request.
2. Extract ALL required parameters from the user's message.
3. If required parameters are missing, ask the user before calling the tool.
4. Chain tool calls in the correct order when multiple steps are needed.
5. Explain what each tool call will accomplish before executing it.
"""


def _ensure_output_format(prompt: str, task: str) -> str:
    """Asegura que el prompt del modelo fuente contiene el bloque de formato de salida
    necesario para que el pipeline de evaluación funcione."""

    if task == "classification":
        # ¿Ya define una estructura JSON de salida?
        if '"category"' in prompt and '"subcategory"' in prompt and '"priority"' in prompt:
            log.info("El prompt fuente ya incluye definición de formato JSON de salida.")
            return prompt
        if re.search(r"(primary_category|category|subcategory|priority|sentiment)", prompt, re.I) \
                and "json" in prompt.lower():
            log.info("El prompt fuente parece definir formato de salida — se mantiene tal cual.")
            return prompt
        log.warning("El prompt fuente no tiene formato de salida JSON explícito — se añade el bloque requerido.")
        return prompt.rstrip() + "\n" + _CLASSIFICATION_OUTPUT_BLOCK

    elif task == "dialog":
        if re.search(r"(follow.?up|context.?gap|question)", prompt, re.I):
            log.info("El prompt fuente de diálogo ya contiene guías de follow-up.")
            return prompt
        log.warning("El prompt fuente de diálogo no tiene guías de follow-up — se añade bloque.")
        return prompt.rstrip() + "\n" + _DIALOG_OUTPUT_BLOCK

    elif task == "rag":
        if re.search(r"(ground|context|retriev|cite|passage)", prompt, re.I):
            log.info("El prompt fuente de RAG ya contiene guías de grounding.")
            return prompt
        log.warning("El prompt fuente de RAG no tiene guías de grounding — se añade bloque.")
        return prompt.rstrip() + "\n" + _RAG_OUTPUT_BLOCK

    elif task == "tool_calling":
        if re.search(r"(tool|function|parameter|invoke|call)", prompt, re.I):
            log.info("El prompt fuente de tool_calling ya contiene guías de tools.")
            return prompt
        log.warning("El prompt fuente de tool_calling no tiene guías de tools — se añade bloque.")
        return prompt.rstrip() + "\n" + _TOOL_CALLING_OUTPUT_BLOCK

    # General: no requiere formato especial
    return prompt


# ===================================================================
# Target model prompt generation (model-specific guidance)
# ===================================================================


def _build_target_generation_meta_prompt(
    topic: str,
    task: str,
    source_prompt: str,
    target_model: str,
    model_family: Optional[str] = None,
    categories: Optional[List[str]] = None,
    deployment_name: Optional[str] = None,
) -> str:
    """Meta-prompt for the AI to generate a target-model prompt from a source prompt.

    Uses :func:`src.utils.model_guidance.get_guidance` for two-tier
    (family + deployment-specific) guidance.
    """

    guidance = _get_model_guidance(target_model, deployment_name=deployment_name, model_family=model_family)

    # Descriptive model label for the meta-prompt
    model_label = target_model.upper()
    if deployment_name:
        model_label = f"{target_model.upper()} (deployment: {deployment_name})"

    task_descriptions = {
        "classification": (
            "a CLASSIFICATION system prompt that:\n"
            "- Defines categories, subcategories, priority levels, and sentiments\n"
            "- Produces structured JSON output\n"
            "- Includes entity extraction (names, IDs, amounts, dates)\n"
            "- Generates appropriate follow-up questions\n"
            "- Adapts the taxonomy and examples to the given TOPIC"
        ),
        "dialog": (
            "a DIALOG / CONVERSATION AGENT system prompt that:\n"
            "- Guides multi-turn conversations with context tracking\n"
            "- Identifies information gaps and asks targeted follow-up questions\n"
            "- Maintains professional tone appropriate to the topic\n"
            "- Handles escalation and resolution flows\n"
            "- Adapts conversation style and knowledge to the given TOPIC"
        ),
        "rag": (
            "a RAG (Retrieval-Augmented Generation) system prompt that:\n"
            "- Grounds all responses in provided context passages\n"
            "- Refuses to hallucinate beyond available evidence\n"
            "- Cites relevant passages or sections\n"
            "- Handles conflicting or incomplete context gracefully\n"
            "- Adapts domain knowledge to the given TOPIC"
        ),
        "tool_calling": (
            "a TOOL CALLING / FUNCTION CALLING agent system prompt that:\n"
            "- Selects appropriate tools based on user intent\n"
            "- Extracts required parameters accurately from queries\n"
            "- Chains multiple tool calls when needed\n"
            "- Handles missing parameters by asking clarifying questions\n"
            "- Adapts tool usage patterns to the given TOPIC"
        ),
    }

    cat_block = ""
    if categories:
        cat_list = "\n".join(f"  - {c}" for c in categories)
        cat_block = (
            f"\n## MANDATORY CATEGORY TAXONOMY (CRITICAL — DO NOT CHANGE)\n"
            f"You MUST use EXACTLY these primary category codes.\n"
            f"Copy each code CHARACTER-BY-CHARACTER — do NOT rename, paraphrase,\n"
            f"merge, split, abbreviate, or invent new categories:\n\n"
            f"{cat_list}\n\n"
            f"These are the ONLY valid primary_category values.\n"
            f"You may freely create subcategories, descriptions, and examples\n"
            f"adapted to this model's style, but the primary category codes\n"
            f"MUST be identical to the list above.\n"
        )

    return f"""Generate a production-ready system prompt for the following scenario:

## TOPIC
{topic}

## TARGET MODEL
{model_label} — follow these guidelines:
{guidance}

## TASK TYPE
Create {task_descriptions.get(task, task)}

## REFERENCE (source prompt for the SAME topic — adapt the STYLE to the
target model's best practices but keep the EXACT SAME primary category codes
and domain knowledge)
{source_prompt[:6000]}

## REQUIREMENTS
1. The prompt must be fully self-contained (no placeholders left)
2. Keep EXACTLY the same primary category codes as the reference — do NOT
   rename, merge, split, or invent new categories
3. Adapt subcategories, descriptions, examples, and formatting to the
   {model_label} style guidelines above
4. Keep the same structural quality as the reference
5. The JSON output schema MUST be compatible with the reference prompt's schema
   (same field names: category, subcategory, priority, sentiment, confidence,
   entities, follow_up_questions, reasoning_summary)
6. Output ONLY the system prompt content — no wrapper, no explanation
{cat_block}
"""


def _fix_target_categories(
    client: AzureOpenAIClient,
    target_prompt: str,
    target_categories: List[str],
    generator_model: str,
    target_model: str = "gpt5",
) -> str:
    """Regenera el prompt del modelo destino con enforcement más estricto de las categorías."""
    fix_prompt = (
        f"The following system prompt was generated for a {target_model.upper()} classification agent, "
        "but it uses WRONG category codes. Rewrite it so that the EXACT primary "
        "category codes listed below are used AS-IS (copy verbatim, do NOT rename).\n\n"
        "MANDATORY PRIMARY CATEGORY CODES (use these EXACTLY):\n"
        + "\n".join(f"  - {c}" for c in target_categories)
        + "\n\nOriginal prompt to fix:\n" + target_prompt[:6000]
        + "\n\nReturn ONLY the corrected system prompt. Keep the same structure, "
        "descriptions, and subcategory style — just replace ALL primary category "
        "codes with the mandatory ones above."
    )
    res = client.complete(
        messages=[
            {"role": "system", "content": "You are an expert prompt engineer. Fix the category codes as instructed. Output ONLY the corrected prompt."},
            {"role": "user", "content": fix_prompt},
        ],
        model_name=generator_model,
    )
    fixed = res.content.strip()
    fixed_cats = _extract_categories_from_prompt(fixed)
    overlap = set(fixed_cats) & set(target_categories)
    if len(overlap) >= len(target_categories) * 0.5:
        log.info(f"Auto-fix de categorías exitoso: {len(overlap)}/{len(target_categories)} alineadas.")
        return fixed
    log.warning("Auto-fix no mejoró el solapamiento — se usa el prompt original del modelo destino.")
    return target_prompt


def generate_target_prompt(
    client: AzureOpenAIClient,
    topic: str,
    task: str,
    source_prompt: str,
    generator_model: str,
    target_model: str = "gpt5",
    model_family: Optional[str] = None,
    deployment_name: Optional[str] = None,
) -> str:
    """Generate a prompt optimised for *target_model* from a source prompt.

    This is the model-agnostic successor of the old ``generate_gpt5_prompt``.
    """

    categories = None
    if task == "classification":
        categories = _extract_categories_from_prompt(source_prompt)
        if categories:
            log.info(f"Categorías extraídas del prompt fuente ({len(categories)}): {categories}")
        else:
            log.warning("No se pudieron extraer categorías del prompt fuente — el modelo destino definirá las suyas.")

    meta_prompt = _build_target_generation_meta_prompt(
        topic, task, source_prompt, target_model, model_family, categories,
        deployment_name=deployment_name,
    )

    log.info(f"Generando prompt {target_model.upper()} mediante IA...")
    t0 = time.time()
    res = client.complete(
        messages=[
            {
                "role": "system",
                "content": (
                    "You are an expert prompt engineer specialising in Azure OpenAI models. "
                    "You create high-quality system prompts that follow each specific model's "
                    "best practices (not just the model family — consider the concrete "
                    "deployment such as GPT-4.1 vs GPT-4o vs GPT-4.1-mini, or GPT-5.2 vs "
                    "GPT-5.1 reasoning). "
                    "Return ONLY the system prompt content, no explanations, no markdown fences."
                ),
            },
            {"role": "user", "content": meta_prompt},
        ],
        model_name=generator_model,
    )
    elapsed = time.time() - t0
    generated = res.content.strip()
    log.info(f"Prompt {target_model.upper()} generado en {elapsed:.1f}s  ({len(generated)} chars)")

    # Validar que el prompt destino reutiliza las mismas categorías
    if categories:
        target_cats = _extract_categories_from_prompt(generated)
        overlap = set(target_cats) & set(categories)
        if len(overlap) < len(categories) * 0.5:
            log.warning(
                f"Prompt {target_model.upper()} tiene bajo solapamiento de categorías "
                f"({len(overlap)}/{len(categories)}). Intentando auto-fix..."
            )
            generated = _fix_target_categories(
                client, generated, categories, generator_model, target_model,
            )

    return generated


# Backward-compatible aliases
def generate_gpt5_prompt(client, topic, task, gpt4_prompt, generator_model):
    """Deprecated alias — use :func:`generate_target_prompt` instead."""
    return generate_target_prompt(
        client, topic, task, gpt4_prompt, generator_model,
        target_model="gpt5", model_family="gpt5",
    )

_fix_gpt5_categories = lambda client, prompt, cats, gen_model: _fix_target_categories(
    client, prompt, cats, gen_model, "gpt5",
)
# Keep for backward compat with old import in routes.py
_build_gpt5_generation_meta_prompt = lambda topic, task, prompt, cats=None: _build_target_generation_meta_prompt(
    topic, task, prompt, "gpt5", "gpt5", cats,
)


# ===================================================================
# Test data validation
# ===================================================================

def validate_and_fix_test_data(data: list, task: str) -> List[str]:
    """Valida que los datos de prueba tengan el esquema esperado por el
    framework.  Completa campos opcionales que falten.
    Devuelve lista de warnings (vacía = todo OK)."""

    warnings: List[str] = []
    if not isinstance(data, list):
        warnings.append("Los datos de prueba deben ser un array JSON.")
        return warnings
    if len(data) == 0:
        warnings.append("Los datos de prueba están vacíos.")
        return warnings

    sample = data[0]

    if task == "classification":
        required = {"id", "customer_input", "expected_category"}
        missing = required - set(sample.keys())
        if missing:
            warnings.append(f"Campos obligatorios ausentes en classification: {missing}")
        for i, item in enumerate(data):
            if "id" not in item:
                item["id"] = f"CLASS_{i+1:03d}"
            if "scenario" not in item:
                item["scenario"] = item.get("id", f"scenario_{i+1}")
            for field in ("expected_subcategory", "expected_priority", "expected_sentiment"):
                if field not in item:
                    item[field] = ""
            if "context" not in item:
                item["context"] = {}
            if "follow_up_questions_expected" not in item:
                item["follow_up_questions_expected"] = []

    elif task == "dialog":
        required = {"id", "conversation", "context_gaps"}
        missing = required - set(sample.keys())
        if missing:
            warnings.append(f"Campos obligatorios ausentes en dialog: {missing}")
        for i, item in enumerate(data):
            if "id" not in item:
                item["id"] = f"DLG_{i+1:03d}"
            if "scenario" not in item:
                item["scenario"] = item.get("id", f"scenario_{i+1}")
            if "optimal_follow_up" not in item:
                item["optimal_follow_up"] = ""
            if "follow_up_rules" not in item:
                item["follow_up_rules"] = []
            if "expected_resolution_turns" not in item:
                item["expected_resolution_turns"] = 2
            if "category" not in item:
                item["category"] = "general"

    elif task == "rag":
        required = {"id", "query", "context", "ground_truth"}
        missing = required - set(sample.keys())
        if missing:
            warnings.append(f"Campos obligatorios ausentes en rag: {missing}")
        for i, item in enumerate(data):
            if "id" not in item:
                item["id"] = f"RAG_{i+1:03d}"
            if "scenario" not in item:
                item["scenario"] = item.get("id", f"rag_scenario_{i+1}")
            if "expected_behavior" not in item:
                item["expected_behavior"] = "grounded_answer"
            if "complexity" not in item:
                item["complexity"] = "medium"

    elif task == "tool_calling":
        required = {"id", "query", "available_tools", "expected_tool_calls"}
        missing = required - set(sample.keys())
        if missing:
            warnings.append(f"Campos obligatorios ausentes en tool_calling: {missing}")
        for i, item in enumerate(data):
            if "id" not in item:
                item["id"] = f"TC_{i+1:03d}"
            if "scenario" not in item:
                item["scenario"] = item.get("id", f"tool_scenario_{i+1}")
            if "expected_parameters" not in item:
                item["expected_parameters"] = {}
            if "complexity" not in item:
                item["complexity"] = "medium"

    elif task == "general":
        required = {"id", "test_type"}
        missing = required - set(sample.keys())
        if missing:
            warnings.append(f"Campos obligatorios ausentes en general: {missing}")
        for i, item in enumerate(data):
            if "id" not in item:
                item["id"] = f"GEN_{i+1:03d}"
            if "complexity" not in item:
                item["complexity"] = "medium"
            if "prompt" not in item and "conversation" not in item:
                warnings.append(f"Item {i} no tiene ni 'prompt' ni 'conversation'.")

    return warnings


# ===================================================================
# Write archived topic into the solution
# ===================================================================

def write_archived_topic(
    slug: str,
    topic_name: str,
    prompts_map: Dict[str, Any],
    test_data_map: Dict[str, list],
) -> Path:
    """Write an archived topic into the solution structure.

    Args:
        slug: Filesystem-safe topic identifier.
        topic_name: Human-readable topic name.
        prompts_map: Dict keyed by task (classification, dialog, rag,
            tool_calling).  Values can be either:
            - **tuple** ``(source_content, target_content)`` — legacy
              two-model format (written as ``source/`` + ``target/``).
            - **dict** ``{model_key: content, ...}`` — N-model format
              (one directory per model key).
        test_data_map: Up to 5 keys (classification, dialog, general,
            rag, tool_calling) → list of scenario dicts.

    Estructura creada:
        prompts/topics/{slug}/
        ├── {model}/{task}_agent_system.md   (por cada modelo y tarea)
        └── topic.json

        data/synthetic/topics/{slug}/
        └── {task}/{data_filename}.json      (por cada tipo de datos)
    """

    prompt_topic_dir = PROMPTS_TOPICS_DIR / slug
    data_topic_dir   = DATA_TOPICS_DIR / slug

    all_models: set = set()

    # --- Prompts (N models per task type) ---
    for task, value in prompts_map.items():
        prompt_type = TASK_PROMPT_MAP.get(task)
        if not prompt_type:
            log.warning(f"Tipo de tarea desconocido para prompts: {task} — se omite.")
            continue

        # Normalize legacy (tuple) format to dict format
        if isinstance(value, (tuple, list)):
            models_content: Dict[str, str] = {
                "gpt4": value[0],
                "gpt5": value[1],
            }
        elif isinstance(value, dict):
            models_content = value
        else:
            log.warning(f"Formato inesperado para prompts_map[{task}]: {type(value)} — se omite.")
            continue

        for model, content in models_content.items():
            all_models.add(model)
            model_dir = prompt_topic_dir / model
            model_dir.mkdir(parents=True, exist_ok=True)
            prompt_file = model_dir / f"{prompt_type}.md"
            prompt_file.write_text(content, encoding="utf-8")
            log.info(f"  Prompt {model}/{prompt_type}.md  ({len(content)} chars)")

    # --- Test data (one file per task type provided) ---
    for data_task, data_items in test_data_map.items():
        data_filename = DATA_FILE_MAP[data_task]
        data_subdir = data_topic_dir / data_task
        data_subdir.mkdir(parents=True, exist_ok=True)
        data_file = data_subdir / data_filename
        with open(data_file, "w", encoding="utf-8") as f:
            json.dump(data_items, f, indent=2, ensure_ascii=False)
        log.info(f"  Datos {data_task}/{data_filename}  ({len(data_items)} items)")

    # --- topic.json ---
    now = datetime.now().isoformat()
    meta = {
        "topic": topic_name,
        "slug": slug,
        "archived_at": now,
        "prompts_updated_at": now,
        "data_generated_at": now,
        "models": sorted(all_models),
    }
    meta_file = prompt_topic_dir / "topic.json"
    meta_file.write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding="utf-8")
    log.info(f"  Metadata topic.json")

    return prompt_topic_dir


# ===================================================================
# Main
# ===================================================================

def main():
    parser = argparse.ArgumentParser(
        description=(
            "Import an external topic (source prompts + test data) as an "
            "archived topic in the evaluation solution, automatically "
            "generating optimised prompts for all target models.\n\n"
            "Source prompt args (at least one required):\n"
            "  --source-class-prompt        → classification\n"
            "  --source-dialog-prompt       → dialog\n"
            "  --source-rag-prompt          → rag\n"
            "  --source-tool-calling-prompt → tool_calling"
        ),
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--topic", required=True,
        help="Human-readable topic name (e.g. 'Insurance Claims Processing').",
    )

    # --- Source model ---
    parser.add_argument(
        "--source-model", type=str, default="gpt4",
        help="Model key the source prompts are written for (default: gpt4).",
    )

    # --- Target models ---
    parser.add_argument(
        "--target-models", type=str, default=None,
        help=(
            "Comma-separated list of model keys to generate prompts for "
            "(e.g. 'gpt5,gpt4o,gpt41_mini'). "
            "Default: all configured models except source."
        ),
    )

    # --- Source prompts (at least one required) ---
    prompt_group = parser.add_argument_group(
        "Source prompts",
        "At least one is required.  Legacy --gpt4-* aliases are supported.",
    )
    prompt_group.add_argument(
        "--source-class-prompt", "--gpt4-class-prompt",
        type=Path, default=None, dest="source_class_prompt",
        help="Text file (.txt/.md) with the source classification system prompt.",
    )
    prompt_group.add_argument(
        "--source-dialog-prompt", "--gpt4-dialog-prompt",
        type=Path, default=None, dest="source_dialog_prompt",
        help="Text file (.txt/.md) with the source dialog system prompt.",
    )
    prompt_group.add_argument(
        "--source-rag-prompt", "--gpt4-rag-prompt",
        type=Path, default=None, dest="source_rag_prompt",
        help="Text file (.txt/.md) with the source RAG system prompt.",
    )
    prompt_group.add_argument(
        "--source-tool-calling-prompt", "--gpt4-tool-calling-prompt",
        type=Path, default=None, dest="source_tool_prompt",
        help="Text file (.txt/.md) with the source tool-calling system prompt.",
    )

    # --- Test data files (at least one required) ---
    data_group = parser.add_argument_group(
        "Test data",
        "At least one is required.  Up to five may be provided.",
    )
    data_group.add_argument("--class-test-data", type=Path, default=None)
    data_group.add_argument("--dialog-test-data", type=Path, default=None)
    data_group.add_argument("--general-test-data", type=Path, default=None)
    data_group.add_argument("--rag-test-data", type=Path, default=None)
    data_group.add_argument("--tool-calling-test-data", type=Path, default=None)

    # --- Optional settings ---
    parser.add_argument(
        "--config", type=str, default=CONFIG_PATH,
        help="Path to settings.yaml (default: config/settings.yaml).",
    )
    parser.add_argument(
        "--generator-model", type=str, default=GENERATOR_MODEL,
        help=f"Model key used to generate target prompts (default: {GENERATOR_MODEL}).",
    )
    parser.add_argument("--force", action="store_true", help="Overwrite if topic already exists.")
    parser.add_argument("--verbose", action="store_true", help="Enable DEBUG logging.")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    source_model = args.source_model

    # --- Verify at least one source prompt is provided ---
    source_prompt_args = {
        "classification": args.source_class_prompt,
        "dialog":         args.source_dialog_prompt,
        "rag":            args.source_rag_prompt,
        "tool_calling":   args.source_tool_prompt,
    }
    if not any(source_prompt_args.values()):
        parser.error(
            "At least one source prompt is required.\n"
            "  Use --source-class-prompt, --source-dialog-prompt, "
            "--source-rag-prompt\n  and/or --source-tool-calling-prompt."
        )

    # --- Verify at least one test data file is provided ---
    test_data_args = {
        "classification": args.class_test_data,
        "dialog":         args.dialog_test_data,
        "general":        args.general_test_data,
        "rag":            args.rag_test_data,
        "tool_calling":   args.tool_calling_test_data,
    }
    if not any(test_data_args.values()):
        parser.error(
            "At least one test data file is required.\n"
            "  Use --class-test-data, --dialog-test-data, --general-test-data,\n"
            "  --rag-test-data and/or --tool-calling-test-data."
        )

    slug = _slugify(args.topic)

    # --- Resolve target models from config ---
    import yaml
    with open(args.config, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    configured_models = list((cfg.get("azure", {}).get("models", {})).keys())
    # Build a mapping model_key → model_family and deployment_name from config
    model_families: Dict[str, str] = {}
    model_deployments: Dict[str, str] = {}
    for mk, mp in (cfg.get("azure", {}).get("models", {})).items():
        model_families[mk] = mp.get("model_family", _resolve_model_family(mk))
        model_deployments[mk] = mp.get("deployment_name", mk)

    if args.target_models:
        target_models = [m.strip() for m in args.target_models.split(",") if m.strip()]
    else:
        target_models = [m for m in configured_models if m != source_model]

    if not target_models:
        log.error("No target models to generate prompts for.  Check --target-models or config.")
        sys.exit(1)

    # --- Banner ---
    prompt_tasks = ", ".join(t for t, p in source_prompt_args.items() if p)
    data_summary = ", ".join(t for t, p in test_data_args.items() if p)
    targets_str = ", ".join(target_models)
    print(f"""
╔══════════════════════════════════════════════════════════════╗
║        Import Topic — Model Migration Evaluation             ║
╠══════════════════════════════════════════════════════════════╣
║  Topic:    {args.topic:<49s}║
║  Source:   {source_model:<49s}║
║  Targets:  {targets_str:<49s}║
║  Prompts:  {prompt_tasks:<49s}║
║  Slug:     {slug:<49s}║
║  Data:     {data_summary:<49s}║
╚══════════════════════════════════════════════════════════════╝
""")

    # --- Check if topic already exists ---
    existing = PROMPTS_TOPICS_DIR / slug
    if existing.exists() and not args.force:
        log.error(
            f"Topic '{slug}' already exists at {existing}.\n"
            f"  Use --force to overwrite."
        )
        sys.exit(1)

    # --- Load and validate test data files ---
    test_data_map: Dict[str, list] = {}
    for data_task, data_path in test_data_args.items():
        if data_path is None:
            continue
        log.info(f"Loading test data ({data_task})...")
        if not data_path.exists():
            log.error(f"Data file not found: {data_path}")
            sys.exit(1)
        with open(data_path, "r", encoding="utf-8-sig") as f:
            raw = json.load(f)
        if isinstance(raw, dict):
            raw = (
                raw.get("scenarios")
                or next((v for v in raw.values() if isinstance(v, list)), [])
            )
        log.info(f"  {len(raw)} scenarios from {data_path}")
        log.info(f"Validating data schema ({data_task})...")
        warnings = validate_and_fix_test_data(raw, data_task)
        if warnings:
            for w in warnings:
                log.warning(f"  [!] {w}")
            if any("obligatorios" in w for w in warnings):
                log.error(f"Critical schema errors in {data_task} data. Aborting.")
                sys.exit(1)
        else:
            log.info(f"  [OK] {data_task} data schema valid.")
        test_data_map[data_task] = raw

    # --- Create Azure OpenAI client ---
    log.info("Creating Azure OpenAI client...")
    try:
        client = create_client_from_config(args.config)
    except Exception as e:
        log.error(f"Could not create client: {e}")
        sys.exit(1)

    # --- Load and validate source prompts (fast, no LLM) ---
    source_contents: Dict[str, str] = {}
    for task, prompt_path in source_prompt_args.items():
        if prompt_path is None:
            continue
        log.info(f"Loading source prompt ({task})...")
        if not prompt_path.exists():
            log.error(f"Prompt file not found: {prompt_path}")
            sys.exit(1)
        raw_text = prompt_path.read_text(encoding="utf-8")
        log.info(f"  {len(raw_text)} chars from {prompt_path}")
        log.info(f"Validating source prompt ({task}) for evaluation compatibility...")
        source_contents[task] = _ensure_output_format(raw_text, task)

    # --- Generate target-model prompts in parallel ---
    # prompts_map: {task: {model: content}}
    prompts_map: Dict[str, Dict[str, str]] = {}

    async def _generate_all_targets():
        sem = asyncio.Semaphore(5)

        async def _gen_one(task_name: str, src_content: str, tgt_model: str):
            async with sem:
                family = model_families.get(tgt_model, _resolve_model_family(tgt_model))
                deployment = model_deployments.get(tgt_model)
                log.info(f"[parallel] Generating {tgt_model.upper()} prompt ({task_name})...")
                t0 = time.time()
                categories = None
                if task_name == "classification":
                    categories = _extract_categories_from_prompt(src_content)
                meta_prompt = _build_target_generation_meta_prompt(
                    args.topic, task_name, src_content, tgt_model, family, categories,
                    deployment_name=deployment,
                )
                res = await client.complete_async(
                    messages=[
                        {"role": "system", "content": (
                            "You are an expert prompt engineer specialising in Azure OpenAI models. "
                            "You create high-quality system prompts that follow each specific model's "
                            "best practices (not just the model family — consider the concrete "
                            "deployment such as GPT-4.1 vs GPT-4o vs GPT-4.1-mini, or GPT-5.2 vs "
                            "GPT-5.1 reasoning). "
                            "Return ONLY the system prompt content, no explanations, no markdown fences."
                        )},
                        {"role": "user", "content": meta_prompt},
                    ],
                    model_name=args.generator_model,
                )
                generated = res.content.strip()
                elapsed = time.time() - t0
                log.info(
                    f"[parallel] {tgt_model.upper()} ({task_name}) generated in "
                    f"{elapsed:.1f}s ({len(generated)} chars)"
                )
                if categories:
                    tgt_cats = _extract_categories_from_prompt(generated)
                    overlap = set(tgt_cats) & set(categories)
                    if len(overlap) < len(categories) * 0.5:
                        log.warning(
                            f"Low category overlap for {tgt_model} ({task_name}). Auto-fix..."
                        )
                        generated = _fix_target_categories(
                            client, generated, categories, args.generator_model, tgt_model,
                        )
                return (task_name, tgt_model, generated)

        jobs = [
            _gen_one(task, content, tgt)
            for task, content in source_contents.items()
            for tgt in target_models
        ]
        return await asyncio.gather(*jobs)

    if source_contents:
        n_jobs = len(source_contents) * len(target_models)
        t0 = time.time()
        log.info(f"Generating {n_jobs} target prompts in parallel...")
        results = asyncio.run(_generate_all_targets())
        elapsed = time.time() - t0
        log.info(f"All target prompts generated in {elapsed:.1f}s (parallel)")

        # Build prompts_map: {task: {model: content}}
        for task_name, tgt_model, tgt_content in results:
            prompts_map.setdefault(task_name, {})[tgt_model] = tgt_content
        # Include source model content
        for task, content in source_contents.items():
            prompts_map.setdefault(task, {})[source_model] = content

    # --- Write archived topic ---
    log.info("Writing archived topic...")
    topic_dir = write_archived_topic(
        slug=slug,
        topic_name=args.topic,
        prompts_map=prompts_map,
        test_data_map=test_data_map,
    )

    # --- Done ---
    all_models_used = sorted({source_model} | set(target_models))
    prompt_lines = []
    for task in prompts_map:
        prompt_type = TASK_PROMPT_MAP.get(task, task)
        models_str = ", ".join(sorted(prompts_map[task].keys()))
        prompt_lines.append(f"    {prompt_type}  ({models_str})")

    data_lines = []
    for dt, items in test_data_map.items():
        data_lines.append(f"    {dt}: {len(items)} scenarios")

    print(f"""
╔══════════════════════════════════════════════════════════════╗
║                        ✓  COMPLETED                          ║
╠══════════════════════════════════════════════════════════════╣
║                                                              ║
║  Topic archived successfully.                                ║
║                                                              ║
║  Prompts:  prompts/topics/{slug + '/':<38s}║
║  Data:     data/synthetic/topics/{slug + '/':<28s}║
║                                                              ║
║  Generated prompts:                                          ║
""")
    for line in prompt_lines:
        print(f"║  {line:<58s}║")
    print(f"""\
║                                                              ║
║  Loaded data:                                                ║
""")
    for line in data_lines:
        print(f"║  {line:<58s}║")
    print(f"""\
║                                                              ║
║  Next steps:                                                 ║
║    1. Open the web UI:  python app.py                        ║
║    2. Go to the Topics section                               ║
║    3. Activate topic "{slug}"
║    4. Run evaluations and comparisons as usual               ║
║                                                              ║
╚══════════════════════════════════════════════════════════════╝
""")


if __name__ == "__main__":
    main()
